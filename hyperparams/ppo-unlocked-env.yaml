################ Logging/Environment hyperparameters ################
algorithm: "PPO"                     # name of the algorithm, enter "PPO" or "IAAPPO"
env_name: "IntrospectiveEnvUnlocked"         # name of the environment
door_locked: False                   # is the door locked?
size: 9                              # gridworld env size
save_frames: False                   # save frames?
save_model_freq: 279                 # save model frequency (in num update iter - not timesteps)
image_observation: False             # rgb-images as observations?
run_num: 3                           # experiment run number for seeding
################ PPO hyperparameters ################
max_training_timesteps: 500_000      # break training loop if timesteps > max_training_timesteps. 1e6
horizon: 128                         # max timesteps in one update (episode); in other words, M rollout steps
minibatch_size: 128                  # mini batch size
k_epochs: 4                          # update policy for K epochs
eps_clip: 0.2                        # clip parameter for PPO
gamma: 0.99                          # discount factor
lr_actor: 0.0001                     # learning rate for actor
random_seed: 47                      # random seed value
num_envs: 2                          # number of parallel environments; batch size = num_envs * horizon = 256
gae_lambda: 0.8                      # gae lambda
